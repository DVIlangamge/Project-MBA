### Exploratory Data Analytics - ## Done by Shaini

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

Global_superstore_data = pd.read_excel("Global Superstore Lite.xlsx")

#understanding the data set
Global_superstore_data.head()

Global_superstore_data.tail()

Global_superstore_data.shape

Global_superstore_data.info

Global_superstore_data.columns

Global_superstore_data.isnull().sum()

Global_superstore_data.describe()

Global_superstore_data.nunique()

# relationship analysis
corelation = Global_superstore_data.corr()
sns.heatmap(corelation, xticklabels=corelation.columns, yticklabels=corelation.columns, annot=True)

sns.pairplot(Global_superstore_data)

sns.displot(Global_superstore_data['Quantity'])

### Data Pre-processing - ## Done by Sihas

# Reading the dataset
df = pd.read_excel('Global Superstore Lite.xlsx')
print(df.head())

print(df.describe())

print(df.isnull().sum())

df = df.drop('Postal Code', axis=1)

print(df.isnull().sum())

df = df.drop('Row ID', axis=1)
columns_info = df.dtypes
print("Columns and their data types:")
print(columns_info)

duplicate_rows = df[df.duplicated()] #Removing duplicate rows
print("Duplicate Rows except first occurrence:")
print(duplicate_rows)

df = df.round(2) #Rounding off to 2 decimal places
print(df.head())

df = df.sort_values(by='Order Priority', ascending=True).reset_index(drop=True) 
df.to_excel('cleaned_dataset_global.xlsx', index=False)

### Market Basket Analysis - ##Done by Imal and Dulmi

#Reading the cleaned dataset
df1 = pd.read_excel("cleaned_dataset_global (1).xlsx")
df1.head()

# Creating a new column as single_transaction using Customer ID and Order Date
df1["single_transaction"] = df1["Customer ID"].astype(str)+'_'+df1['Order Date'].astype(str)
df1.head()

# Creating a table with the new column and Sub-Category
df2 = pd.crosstab(df1['single_transaction'],df1['Sub-Category'])
df2

## TRAINING DATASET - ## Done by Navoda

###### MBA using Segments to train the dataset
## For Segment 1

s1 = (df[df["Segment"] == "Consumer"]
     .groupby(["Order ID", "Sub-Category"])["Quantity"]
     .sum().unstack().reset_index().fillna(0)
     .set_index("Order ID"))
s1

def encode_units(x):
    if x <=0:
        return 0
    if x >=1:
        return 1

s1_sets = s1.applymap(encode_units)

from mlxtend.frequent_patterns import apriori
from mlxtend.frequent_patterns import association_rules

frequent_itemsets_s1 = apriori(s1_sets, min_support=0.001, use_colnames = True)
rules_s1 = association_rules(frequent_itemsets_s1, metric = "lift", min_threshold=1)
rules_s1

#vis1(heatmap)
heatmap_data = rules_s1.pivot(index='antecedents', columns='consequents', values='lift')
plt.figure(figsize=(5, 4))
sns.heatmap(heatmap_data, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('lift Heatmap of Association Rules')
plt.xlabel('Consequents')
plt.ylabel('Antecedents')
plt.show()

## For Segment 2
s2 = (df[df["Segment"] == "Home Office"]
     .groupby(["Order ID", "Sub-Category"])["Quantity"]
     .sum().unstack().reset_index().fillna(0)
     .set_index("Order ID"))
s2

def encode_units(x):
    if x <=0:
        return 0
    if x >=1:
        return 1

s2_sets = s2.applymap(encode_units)

from mlxtend.frequent_patterns import apriori
from mlxtend.frequent_patterns import association_rules

frequent_itemsets_s2 = apriori(s2_sets, min_support=0.001, use_colnames = True)
rules_s2 = association_rules(frequent_itemsets_s2, metric = "lift", min_threshold=1)
rules_s2

# vis2(heatmap)

heatmap_data = rules_s2.pivot(index='antecedents', columns='consequents', values='lift')
plt.figure(figsize=(5, 4))
sns.heatmap(heatmap_data, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('lift Heatmap of Association Rules')
plt.xlabel('Consequents')
plt.ylabel('Antecedents')
plt.show()


## For Segment 3

s3 = (df[df["Segment"] == "Corporate"]
     .groupby(["Order ID", "Sub-Category"])["Quantity"]
     .sum().unstack().reset_index().fillna(0)
     .set_index("Order ID"))
s3

def encode_units(x):
    if x <=0:
        return 0
    if x >=1:
        return 1

s3_sets = s3.applymap(encode_units)

from mlxtend.frequent_patterns import apriori
from mlxtend.frequent_patterns import association_rules

frequent_itemsets_s3 = apriori(s3_sets, min_support=0.001, use_colnames = True)
rules_s3 = association_rules(frequent_itemsets_s3, metric = "lift", min_threshold=1)
rules_s3

# vis3(heatmap)
heatmap_data = rules_s3.pivot(index='antecedents', columns='consequents', values='lift')
plt.figure(figsize=(10, 8))
sns.heatmap(heatmap_data, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('lift Heatmap of Association Rules')
plt.xlabel('Consequents')
plt.ylabel('Antecedents')
plt.show()

#unique results
f=rules_s3[(rules_s3["lift"]>=1)&(rules_s3["confidence"]>=0.1)]
f

# vis4(scatter)
plt.figure(figsize=(5, 4))
plt.scatter(range(len(f)), f['lift'], c=f['lift'], cmap='coolwarm')
plt.colorbar(label='Lift')
plt.title('Scatter Plot of Lift Values')
plt.xlabel('Association Rule Index')
plt.ylabel('Lift')
plt.show()

## MBA for whole dataset

# Encoding data 
def encode(item_freq):
    res = 0
    if item_freq > 0:
        res = 1
    return res
    
basket_input = df2.applymap(encode)

## SUPPORT

# Apply apriori algorithm for frequent itemsets
from mlxtend.frequent_patterns import apriori
from mlxtend.frequent_patterns import association_rules

frequent_itemsets = apriori(basket_input, min_support = 0.001, use_colnames=True)

frequent_itemsets

## CONFIDENCE

# Generate association rules with the measure confidence
rules1 = association_rules(frequent_itemsets, metric = "confidence", min_threshold=0.01)
rules1

## LIFT

# Generate association rules with lift measure
rules2 = association_rules(frequent_itemsets, metric = "lift", min_threshold=1)

rules2.sort_values(["support", "confidence", "lift"],axis =0, ascending = False)

# Final result vizualization - ## All heatmaps are done by Hewa Prabashini

# Pivot the DataFrame to prepare it for the heatmap
heatmap_data = rules2.pivot(index='antecedents', columns='consequents', values='lift')

# Create the heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(heatmap_data, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('lift Heatmap of Association Rules')
plt.xlabel('Consequents')
plt.ylabel('Antecedents')
plt.show()

#unique results
f1=rules2[(rules2["lift"]>=1)&(rules2["confidence"]>=0.1)]
f1

# unique vizualization
plt.figure(figsize=(5, 4))
plt.scatter(range(len(f1)), f1['lift'], c=f1['lift'], cmap='coolwarm')
plt.colorbar(label='Lift')
plt.title('Scatter Plot of Lift Values')
plt.xlabel('Association Rule Index')
plt.ylabel('Lift')
plt.show()
